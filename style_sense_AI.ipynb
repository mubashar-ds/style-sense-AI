{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ7Ho7a9VIP2"
      },
      "source": [
        "# Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5H564EesakI"
      },
      "outputs": [],
      "source": [
        "!pip install torch diffusers  transformers pillow matplotlib mediapipe opencv-python opencv-contrib-python gradio blip  torchvision requests beautifulsoup4 fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jct6oXUktTRK"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y wget unzip xvfb libxi6 libgconf-2-4\n",
        "!apt-get install -y libappindicator1 fonts-liberation\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get -fy install\n",
        "!rm google-chrome-stable_current_amd64.deb\n",
        "\n",
        "!pip install selenium requests webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DST3pwZt45C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install groq langchain_community sentence_transformers\n",
        "!pip install llama-index-llms-groq\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXDp4A4AVsX2"
      },
      "source": [
        "# Data Scrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3RzzOUzxPXs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import shutil\n",
        "import urllib.parse\n",
        "import concurrent.futures\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torchvision.transforms as transforms\n",
        "from typing import List, Dict\n",
        "from groq import Groq\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Websites to scrape\n",
        "websites = {\n",
        "    'junaidjamshed': 'https://www.junaidjamshed.com/womens/kurti.html?product_list_dir=desc&product_list_order=top_rated',\n",
        "    'khaadi': 'https://pk.khaadi.com/ready-to-wear/essentials/kurta/kurta/?prefn1=filter_categories&prefv1=Kurta&srule=most-popular&start=0&sz=96',\n",
        "}\n",
        "\n",
        "# Keywords to filter images (specific to shirts)\n",
        "keywords = ['shirt', 'kurta', 'kurti']\n",
        "\n",
        "# Folder to save images\n",
        "output_folder = \"scraped_images\"\n",
        "\n",
        "# Clear output folder before scraping\n",
        "if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)  # Delete the folder and its contents\n",
        "os.makedirs(output_folder, exist_ok=True)  # Recreate the folder\n",
        "\n",
        "# Selenium setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.binary_location = \"/usr/bin/google-chrome\"\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# Function to fetch image using requests\n",
        "def fetch_image(img_url):\n",
        "    try:\n",
        "        if img_url and img_url.startswith('http'):\n",
        "            img_data = requests.get(img_url, timeout=10).content\n",
        "            return img_data\n",
        "        else:\n",
        "            logging.warning(f\"Invalid image URL: {img_url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch image {img_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to save images (with concurrency)\n",
        "def save_images(site_name, images):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        img_data_list = list(executor.map(fetch_image, images[:10]))  # Fetch in parallel\n",
        "\n",
        "    for i, img_data in enumerate(img_data_list):\n",
        "        if img_data:\n",
        "            img_name = f\"{site_name}shirt{i + 1}.jpg\"\n",
        "            img_path = os.path.join(output_folder, img_name)\n",
        "            try:\n",
        "                with open(img_path, 'wb') as img_file:\n",
        "                    img_file.write(img_data)\n",
        "                logging.info(f\"Saved {img_name}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to save image {img_name}: {e}\")\n",
        "\n",
        "# Function to scrape images from Junaid Jamshed using Selenium\n",
        "def scrape_images_junaidjamshed(site_name, url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "\n",
        "        # Scroll to load all images (limited number of scrolls)\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        scroll_limit = 5  # Limit the number of scrolls\n",
        "        scroll_count = 0\n",
        "\n",
        "        while scroll_count < scroll_limit:\n",
        "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
        "            time.sleep(2)  # Wait for images to load\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "            scroll_count += 1\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "        images = []\n",
        "        img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
        "        seen_urls = set()  # To track already seen images\n",
        "\n",
        "        for img in img_elements:\n",
        "            img_url = img.get_attribute('src') or img.get_attribute('data-src') or img.get_attribute('srcset')\n",
        "            alt_text = img.get_attribute('alt')\n",
        "\n",
        "            if img_url and img_url.startswith('data:image') or img_url in seen_urls:\n",
        "                continue  # Skip base64 images or duplicates\n",
        "\n",
        "            seen_urls.add(img_url)\n",
        "\n",
        "            if alt_text and any(keyword.lower() in alt_text.lower() for keyword in keywords):\n",
        "                images.append(img_url)\n",
        "\n",
        "        save_images(site_name, images)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {site_name}: {e}\")\n",
        "\n",
        "# Function to scrape images from Khaadi using BeautifulSoup\n",
        "def scrape_images_khaadi(site_name, url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        images = []\n",
        "        img_elements = soup.find_all('img')\n",
        "\n",
        "        seen_urls = set()\n",
        "        for img in img_elements:\n",
        "            img_url = img.get('src') or img.get('data-src')\n",
        "            alt_text = img.get('alt')\n",
        "\n",
        "            # Skip base64 images\n",
        "            if img_url and img_url.startswith('data:image'):\n",
        "                continue\n",
        "\n",
        "            # Handle relative URLs\n",
        "            img_url = urllib.parse.urljoin(url, img_url)\n",
        "\n",
        "            # Skip duplicate URLs\n",
        "            if img_url in seen_urls:\n",
        "                continue\n",
        "            seen_urls.add(img_url)\n",
        "\n",
        "            # Filter images by keywords in alt text\n",
        "            if alt_text and any(keyword.lower() in alt_text.lower() for keyword in keywords):\n",
        "                images.append(img_url)\n",
        "\n",
        "        save_images(site_name, images)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {site_name}: {e}\")\n",
        "\n",
        "# Load all images from the 'scraped_images' folder\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = preprocess_image(image_path)\n",
        "            images.append((image, image_path))  # Store image and path tuple\n",
        "    return images\n",
        "\n",
        "# Preprocess input images\n",
        "def preprocess_image(image_path, size=(512, 512)):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    resize_transform = transforms.Resize(size)\n",
        "    return resize_transform(image)\n",
        "\n",
        "# Main scraping function\n",
        "def scrape_data():\n",
        "    all_images = []\n",
        "    for site_name, url in websites.items():\n",
        "        logging.info(f\"Scraping {site_name} for shirts...\")\n",
        "        if site_name == 'junaidjamshed':\n",
        "            scrape_images_junaidjamshed(site_name, url)\n",
        "        elif site_name == 'khaadi':\n",
        "            scrape_images_khaadi(site_name, url)\n",
        "\n",
        "    # Reload the gallery images after scraping\n",
        "    all_images = load_images_from_folder(output_folder)\n",
        "    return all_images\n",
        "\n",
        "# Start scraping\n",
        "images = scrape_data()\n",
        "logging.info(f\"Scraping complete. Images saved: {[img[1] for img in images]}\")\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZefWnbWWsy"
      },
      "source": [
        "# Generative Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL1bgUSkxlOU"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "\n",
        "# Thread-safe variables\n",
        "images_lock = threading.Lock()\n",
        "descriptions_lock = threading.Lock()\n",
        "\n",
        "# Set up models and processor (Preload for efficiency)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44UMt2L9VHGn"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torchvision.transforms as transforms\n",
        "from typing import List, Dict\n",
        "from groq import Groq\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from functools import lru_cache  # Using lru_cache for caching\n",
        "\n",
        "# Set up Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "client = Groq()  # Initialize Groq API client\n",
        "DEFAULT_MODEL = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "# Load BLIP model and processor for image captioning\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Function to create assistant message format\n",
        "def assistant(content: str):\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "# Function to create user message format\n",
        "def user(content: str):\n",
        "    return {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "# Function for chat completion with Groq\n",
        "def chat_completion(messages: List[Dict], model=DEFAULT_MODEL, temperature=0.6, top_p=0.9) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Preprocess input images\n",
        "def preprocess_image(image_path, size=(512, 512)):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    resize_transform = transforms.Resize(size)\n",
        "    return resize_transform(image)\n",
        "\n",
        "# Load all images from the 'scraped_images' folder\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = preprocess_image(image_path)\n",
        "            images.append((image, image_path))  # Store image and path tuple\n",
        "    return images\n",
        "\n",
        "# Generate a description for an image using BLIP and save it as a .txt file\n",
        "def generate_description_with_blip(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    out = blip_model.generate(**inputs)\n",
        "    description = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # Save the description to a .txt file in the same folder as the image\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    description_path = os.path.join(scraped_images_folder, f\"{base_name}.txt\")\n",
        "    with open(description_path, \"w\") as desc_file:\n",
        "        desc_file.write(description)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Path to the folder where the scraped images are saved\n",
        "scraped_images_folder = \"scraped_images\"\n",
        "\n",
        "# Global variables to hold loaded images and descriptions\n",
        "images = []\n",
        "descriptions = {}\n",
        "\n",
        "# Manually cache the descriptions to avoid re-computation\n",
        "@lru_cache(maxsize=128)\n",
        "def generate_description_with_blip_cached(image_path):\n",
        "    return generate_description_with_blip(image_path)\n",
        "\n",
        "# Function to save uploaded image and description without re-fetching all data\n",
        "def save_uploaded_data(uploaded_image, description):\n",
        "    global images, descriptions\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Save the image to the 'scraped_images' folder\n",
        "        image_path = os.path.join(scraped_images_folder, os.path.basename(uploaded_image))\n",
        "        try:\n",
        "            os.rename(uploaded_image, image_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error renaming file: {e}\")\n",
        "            return gr.update()\n",
        "\n",
        "        # Generate description using BLIP if no description is provided\n",
        "        if not description:\n",
        "            description = generate_description_with_blip_cached(image_path)\n",
        "\n",
        "        # Preprocess and add the new image to the existing images list (without reloading all images)\n",
        "        new_image = preprocess_image(image_path)\n",
        "        images.append((new_image, image_path))\n",
        "\n",
        "        # Add the new description to the existing descriptions dictionary\n",
        "        descriptions[os.path.basename(image_path)] = description\n",
        "\n",
        "        # Update the gallery with the new image only\n",
        "        return gr.update(value=[img[0] for img in images])  # Only update the gallery with the current images\n",
        "    return gr.update()\n",
        "\n",
        "# Load the Stable Diffusion pipeline\n",
        "def load_pipeline():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "\n",
        "# Sync sliders to ensure they sum to 1\n",
        "def sync_sliders(value):\n",
        "    return 1 - value\n",
        "\n",
        "# Blend and generate image using the Stable Diffusion pipeline\n",
        "def blend_and_generate_image(image_paths, alpha1, alpha2, generated_prompt):\n",
        "    if not image_paths or len(image_paths) < 1:\n",
        "        return None\n",
        "\n",
        "    alpha1 = max(0, min(1, alpha1))\n",
        "    alpha2 = max(0, min(1, alpha2))\n",
        "    if alpha1 + alpha2 != 1:\n",
        "        alpha2 = 1 - alpha1\n",
        "\n",
        "    image1 = Image.open(image_paths[0]).convert(\"RGBA\")\n",
        "    blended_image = image1\n",
        "    pipe = load_pipeline()\n",
        "    output_image = pipe(\n",
        "        prompt=generated_prompt,\n",
        "        image=blended_image.convert(\"RGB\"),\n",
        "        strength=0.80,\n",
        "        guidance_scale=7.5,\n",
        "        num_inference_steps=50,\n",
        "        generator=torch.manual_seed(42),\n",
        "    ).images[0]\n",
        "\n",
        "    return output_image\n",
        "\n",
        "# Generate a creative prompt based on selected images' descriptions\n",
        "def generate_prompt_from_selected_images(image_paths):\n",
        "    selected_descriptions = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image_name = os.path.basename(image_path)\n",
        "        description = descriptions.get(image_name, \"\")\n",
        "        if not description:\n",
        "            description = generate_description_with_blip_cached(image_path)\n",
        "            descriptions[image_name] = description\n",
        "        selected_descriptions.append(description)\n",
        "\n",
        "    if selected_descriptions:\n",
        "        groq_prompt = chat_completion([user(f'Combine the provided descriptions creatively to design a unique kurti: {\", \".join(selected_descriptions)}. Emphasize the design details, patterns, and fabric texture. Do not include descriptive comments about the body of model. Ensure that the model face is completely cropped out, with no visible facial features.')])\n",
        "        return groq_prompt\n",
        "    else:\n",
        "        return \"No descriptions found for the selected images.\"\n",
        "\n",
        "# Gradio app\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        with gr.Row():\n",
        "            gr.Markdown(\"### Select images and generate a new image based on descriptions and blending\")\n",
        "\n",
        "        with gr.Row():\n",
        "            fetch_data_button = gr.Button(\"Fetch New Data\")\n",
        "            upload_image = gr.File(label=\"Upload Image\", type=\"filepath\", file_types=[\".jpg\"])\n",
        "            upload_button = gr.Button(\"Upload\")\n",
        "\n",
        "        with gr.Row():\n",
        "            gallery = gr.Gallery(label=\"Loaded Images\", value=[], interactive=True, columns=4, height=\"auto\")\n",
        "\n",
        "        with gr.Row():\n",
        "            image1_display = gr.Textbox(label=\"First Selected Image Path\", interactive=False)\n",
        "            image2_display = gr.Textbox(label=\"Second Selected Image Path\", interactive=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            generated_prompt_display = gr.Textbox(label=\"Generated Prompt\", interactive=True, lines=3)\n",
        "\n",
        "        with gr.Row():\n",
        "            alpha1_slider = gr.Slider(0, 1, value=0.5, step=0.01, label=\"Weight of First Image\")\n",
        "            alpha2_slider = gr.Slider(0, 1, value=0.5, step=0.01, label=\"Weight of Second Image\")\n",
        "\n",
        "        with gr.Row():\n",
        "            output_generated_image = gr.Image(label=\"Generated Image\", type=\"pil\", interactive=False)\n",
        "\n",
        "        selected_images = []\n",
        "\n",
        "        def handle_selection(evt: gr.SelectData):\n",
        "            selected_path = images[evt.index][1]\n",
        "            if len(selected_images) < 2:\n",
        "                selected_images.append(selected_path)\n",
        "            else:\n",
        "                selected_images.pop(0)\n",
        "                selected_images.append(selected_path)\n",
        "\n",
        "            if len(selected_images) > 0:\n",
        "                generated_prompt = generate_prompt_from_selected_images(selected_images)\n",
        "                return selected_images[0] if len(selected_images) > 0 else \"\", selected_images[1] if len(selected_images) > 1 else \"\", generated_prompt\n",
        "            return \"\", \"\", \"\"\n",
        "\n",
        "        gallery.select(handle_selection, None, [image1_display, image2_display, generated_prompt_display])\n",
        "\n",
        "        alpha1_slider.change(sync_sliders, inputs=alpha1_slider, outputs=alpha2_slider)\n",
        "        alpha2_slider.change(sync_sliders, inputs=alpha2_slider, outputs=alpha1_slider)\n",
        "\n",
        "        def fetch_new_data_and_update_gallery():\n",
        "            global images, descriptions\n",
        "            updated_images = load_images_from_folder(scraped_images_folder)\n",
        "            descriptions = {os.path.basename(img[1]): generate_description_with_blip_cached(img[1]) for img in updated_images}\n",
        "            images = updated_images\n",
        "            return gr.update(value=[img[0] for img in updated_images])\n",
        "\n",
        "        fetch_data_button.click(fetch_new_data_and_update_gallery, None, gallery)\n",
        "\n",
        "        upload_button.click(save_uploaded_data, [upload_image], gallery)\n",
        "\n",
        "        def blend_and_generate(image1_path, image2_path, alpha1, alpha2, generated_prompt):\n",
        "            return blend_and_generate_image([image1_path, image2_path], alpha1, alpha2, generated_prompt)\n",
        "\n",
        "        generate_button = gr.Button(\"Generate Image\")\n",
        "        generate_button.click(blend_and_generate, [image1_display, image2_display, alpha1_slider, alpha2_slider, generated_prompt_display], output_generated_image)\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_ui()\n",
        "    app.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}